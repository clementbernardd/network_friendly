\relax 
\providecommand\hyper@newdestlabel[2]{}
\bbl@cs{beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Problem}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Catalogue}{2}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Recommendation System}{2}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Caching cost}{2}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{User models}{2}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Recommendation process}{3}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Reinforcement learning}{3}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Definition}{3}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Reinforcement learning}{3}{section*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Elements of Reinforcement Learning}{3}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Markov Decision Processes}{3}{subsubsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Agent}{3}{section*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Environment}{3}{section*.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Reward}{3}{section*.10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Action}{3}{section*.11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{State}{3}{section*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{}{4}{section*.13}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Agent-environment interactions in a MDP\relax }}{4}{figure.caption.14}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{}{4}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Returns, policy and value function}{4}{subsubsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Return}{4}{section*.16}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Expected return}{4}{section*.17}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Policy }{5}{section*.18}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Value function}{5}{section*.19}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Action-value function}{5}{section*.20}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Q-Learning}{5}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Optimality}{5}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{}{5}{section*.21}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Order in term of policy}{5}{section*.22}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Optimal policy}{5}{section*.23}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Bellman optimality equations}{5}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Q learning algorithm}{6}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{}{6}{section*.24}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Q learning algorithm\relax }}{6}{figure.caption.25}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Exploration and Exploitation ($\epsilon $-greedy)}{7}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Convergence criteria}{7}{subsection.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Hyperparameters}{7}{subsection.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7}Results}{8}{subsection.4.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.7.1}Q-table}{8}{subsubsection.4.7.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Q-tables for $\gamma = 0$ \relax }}{9}{figure.caption.26}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:example}{{3}{9}{Q-tables for $\gamma = 0$ \relax }{figure.caption.26}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Classic recommender }}}{9}{subfigure.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Restricted recommender}}}{9}{subfigure.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{}{9}{section*.27}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{}{9}{section*.28}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.7.2}Discounted $\gamma $ parameter}{9}{subsubsection.4.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{}{9}{section*.29}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{}{9}{section*.31}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Q-table for $\gamma = 0$ (left) and $\gamma = 0.9$ (right) without constraints on the recommendations\relax }}{10}{figure.caption.30}\protected@file@percent }
\newlabel{fig:my_label}{{4}{10}{Q-table for $\gamma = 0$ (left) and $\gamma = 0.9$ (right) without constraints on the recommendations\relax }{figure.caption.30}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.7.3}Rewards and penalties}{10}{subsubsection.4.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Rewards}{10}{section*.32}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Penalties}{10}{section*.33}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{}{10}{section*.34}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{}{10}{section*.36}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Rewards and penalties for $\gamma = 0$ and $\gamma = 0.9$ without restrictions on recommendation \relax }}{11}{figure.caption.35}\protected@file@percent }
\newlabel{fig:example}{{5}{11}{Rewards and penalties for $\gamma = 0$ and $\gamma = 0.9$ without restrictions on recommendation \relax }{figure.caption.35}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Rewards}}}{11}{subfigure.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Penalties}}}{11}{subfigure.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.7.4}Specific user}{11}{subsubsection.4.7.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Sum of the optimised policy and our policy with constraints with Q-learning algorithm\relax }}{11}{figure.caption.37}\protected@file@percent }
\newlabel{fig:my_label}{{6}{11}{Sum of the optimised policy and our policy with constraints with Q-learning algorithm\relax }{figure.caption.37}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.7.5}Time of convergence}{12}{subsubsection.4.7.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Criteria of convergence}{12}{section*.38}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Evolution of the maximum difference of q-table ($\gamma = 0.9$) \relax }}{12}{figure.caption.39}\protected@file@percent }
\newlabel{fig:my_label}{{7}{12}{Evolution of the maximum difference of q-table ($\gamma = 0.9$) \relax }{figure.caption.39}{}}
\@writefile{toc}{\contentsline {paragraph}{Time of convergence}{12}{section*.40}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{}{12}{section*.42}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Time and epochs for convergence depending on the catalogue size\relax }}{13}{figure.caption.41}\protected@file@percent }
\newlabel{fig:my_label}{{8}{13}{Time and epochs for convergence depending on the catalogue size\relax }{figure.caption.41}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Deep Q-learning}{13}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Function approximation}{13}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{}{13}{section*.43}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Function approximation summary\relax }}{13}{figure.caption.44}\protected@file@percent }
\newlabel{fig:my_label}{{9}{13}{Function approximation summary\relax }{figure.caption.44}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Training part}{13}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{}{13}{section*.45}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Target}{14}{section*.46}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Loss}{14}{section*.47}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Memory replay}{14}{section*.48}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Summary of deep Q-learning for reinforcement learning\relax }}{14}{figure.caption.49}\protected@file@percent }
\newlabel{fig:my_label}{{10}{14}{Summary of deep Q-learning for reinforcement learning\relax }{figure.caption.49}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}State representation}{15}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{}{15}{section*.50}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{One hot encoding }{15}{section*.51}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{U matrix }{15}{section*.52}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Rewards}{15}{section*.53}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Valuable }{15}{section*.54}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{}{16}{section*.55}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Different state representations for deep q learning algorithm \relax }}{16}{figure.caption.56}\protected@file@percent }
\newlabel{fig:example}{{11}{16}{Different state representations for deep q learning algorithm \relax }{figure.caption.56}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {One hot encoding}}}{16}{subfigure.11.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Rewards}}}{16}{subfigure.11.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {U matrix}}}{16}{subfigure.11.3}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {U one hot encoded matrix}}}{16}{subfigure.11.4}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {Valuable}}}{16}{subfigure.11.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Algorithm}{17}{subsection.5.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Deep Q-learning pseudo code \relax }}{17}{figure.caption.57}\protected@file@percent }
\newlabel{fig:my_label}{{12}{17}{Deep Q-learning pseudo code \relax }{figure.caption.57}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Results}{17}{subsection.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Environment}{17}{section*.58}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Q-learning results}{17}{section*.60}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Reward matrix\relax }}{18}{figure.caption.59}\protected@file@percent }
\newlabel{fig:my_label}{{13}{18}{Reward matrix\relax }{figure.caption.59}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Rewards with Q-learning algorithm\relax }}{18}{figure.caption.61}\protected@file@percent }
\newlabel{fig:my_label}{{14}{18}{Rewards with Q-learning algorithm\relax }{figure.caption.61}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.1}Linear model}{18}{subsubsection.5.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{}{18}{section*.62}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Rewards}{19}{section*.63}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Rewards for different representation of the states and for Q-learning, for $\gamma $ = 0 (left) and $\gamma $= 0.9 (right) \relax }}{19}{figure.caption.64}\protected@file@percent }
\newlabel{fig:example}{{15}{19}{Rewards for different representation of the states and for Q-learning, for $\gamma $ = 0 (left) and $\gamma $= 0.9 (right) \relax }{figure.caption.64}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Rewards with $\gamma $ = 0}}}{19}{subfigure.15.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Rewards with $\gamma $ = 0.9}}}{19}{subfigure.15.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Loss}{19}{section*.65}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Loss for different representation of the states, for $\gamma $ = 0 (left) and $\gamma $= 0.9 (right) for linear model\relax }}{20}{figure.caption.66}\protected@file@percent }
\newlabel{fig:example}{{16}{20}{Loss for different representation of the states, for $\gamma $ = 0 (left) and $\gamma $= 0.9 (right) for linear model\relax }{figure.caption.66}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Loss with $\gamma $ = 0}}}{20}{subfigure.16.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Loss with $\gamma $ = 0.9}}}{20}{subfigure.16.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Q-tables}{20}{section*.67}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Q-tables for different representation of states and $\gamma = 0$ : hot encoding, U hot encoding, valuable and rewards\relax }}{20}{figure.caption.68}\protected@file@percent }
\newlabel{fig:my_label}{{17}{20}{Q-tables for different representation of states and $\gamma = 0$ : hot encoding, U hot encoding, valuable and rewards\relax }{figure.caption.68}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.2}Fully connected model}{21}{subsubsection.5.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{}{21}{section*.69}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Architecture}{21}{section*.70}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Rewards and loss}{21}{section*.71}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Reward and Loss for different representation of the states, for $\gamma $ = 0.9 after 5 000 epochs with a fully connected model\relax }}{21}{figure.caption.72}\protected@file@percent }
\newlabel{fig:example}{{18}{21}{Reward and Loss for different representation of the states, for $\gamma $ = 0.9 after 5 000 epochs with a fully connected model\relax }{figure.caption.72}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Loss}}}{21}{subfigure.18.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Rewards}}}{21}{subfigure.18.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Q-tables}{21}{section*.73}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Q-tables for different representation of states and $\gamma = 0.9$ with fully connected model for : hot encoding, U hot encoding, valuable and rewards\relax }}{21}{figure.caption.74}\protected@file@percent }
\newlabel{fig:my_label}{{19}{21}{Q-tables for different representation of states and $\gamma = 0.9$ with fully connected model for : hot encoding, U hot encoding, valuable and rewards\relax }{figure.caption.74}{}}
\@writefile{toc}{\contentsline {paragraph}{Limits}{22}{section*.75}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Double Q-learning}{22}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Target network}{22}{section*.76}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Schema of double-Q-learning \relax }}{22}{figure.caption.77}\protected@file@percent }
\newlabel{fig:my_label}{{20}{22}{Schema of double-Q-learning \relax }{figure.caption.77}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}State representation}{23}{subsection.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.1}Multiple}{23}{subsubsection.6.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.2}Multiple valuable}{23}{subsubsection.6.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Improvement}{24}{subsection.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Reward and Loss for different target update with 'multiple' as representation and $\gamma $ = 0, linear model \relax }}{24}{figure.caption.78}\protected@file@percent }
\newlabel{fig:example}{{21}{24}{Reward and Loss for different target update with 'multiple' as representation and $\gamma $ = 0, linear model \relax }{figure.caption.78}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Rewards}}}{24}{subfigure.21.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Loss}}}{24}{subfigure.21.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Results}{24}{subsection.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.1}Rewards and Loss}{24}{subsubsection.6.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Reward and Loss with double Q-learning for different representation states, $\gamma = 0.9$ and fully connected of size 300 \relax }}{25}{figure.caption.79}\protected@file@percent }
\newlabel{fig:example}{{22}{25}{Reward and Loss with double Q-learning for different representation states, $\gamma = 0.9$ and fully connected of size 300 \relax }{figure.caption.79}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Rewards}}}{25}{subfigure.22.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Loss}}}{25}{subfigure.22.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.2}Q-table}{25}{subsubsection.6.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Q-table for different representation and $\gamma = 0.9$ : multiple valuable, multiple and rewards\relax }}{25}{figure.caption.80}\protected@file@percent }
\newlabel{fig:my_label}{{23}{25}{Q-table for different representation and $\gamma = 0.9$ : multiple valuable, multiple and rewards\relax }{figure.caption.80}{}}
\bibcite{Giannakas, Theodoros and Sermpezis, Pavlos and Spyropoulos, Thrasyvoulos}{1}
\bibcite{Richard S. Sutton and Andrew G. Barto}{2}
\bibcite{Pytorch}{3}
\bibcite{Pong from Pixels}{4}
\bibcite{A Hands-On Introduction to Deep Q-Learning using OpenAI Gym in Python}{5}
\bibcite{An introduction to Deep Q-Learning: letâ€™s play Doom}{6}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Q-table for 'multiple' representation for state 0\relax }}{26}{figure.caption.81}\protected@file@percent }
\newlabel{fig:my_label}{{24}{26}{Q-table for 'multiple' representation for state 0\relax }{figure.caption.81}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{26}{section.7}\protected@file@percent }
\bibcite{Getting Started with Gym}{7}
